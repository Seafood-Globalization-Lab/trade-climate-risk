for (i in ac_components$ac_component) {# iterate through each component
# number of variables per adaptive capacity variable
num_vars <- ac_components$num_vars[ac_components$ac_component == i]
pval = 1
for (j in 1:num_vars) { # Iterate through each variable
# Reset modifier
modifier = 0
pval = 1
while (pval >= 0.05 &
(modifier + 50) / ((modifier + 50)+ (50 * (num_vars - 1))) <= 0.90) {
# Create the concentration vector with the correct number of 50's
concentration <- rep(50, num_vars)
concentration[j] <- concentration[j] + modifier
# Generate 1000 samples from the Dirichlet distribution
samples <- rdirichlet(1000, concentration)
# Take a random sample from the Dirichlet distribution
dirichlet_probs <- samples[sample(1:nrow(samples), 1), ]
modifier <- modifier + 5
# assets
if (i == "assets") {
d01 <-  dirichlet_probs[which(ac_variables[ac_variables$ac_component == i,]$ac_variable == "gdp_scaled")]
d02 <-  dirichlet_probs[which(ac_variables[ac_variables$ac_component == i,]$ac_variable == "gdp_trade_scaled")]
d03 <-  dirichlet_probs[which(ac_variables[ac_variables$ac_component == i,]$ac_variable == "sanitation_scaled")]
} else {
d01 <- 1
d02 <- 1
d03 <- 1
}
# flexibility
if (i == "flexibility") {
d04 <- concentration[which(ac_variables[ac_variables$ac_component == i,]$ac_variable == "supermarkets_scaled")]
d05 <- concentration[which(ac_variables[ac_variables$ac_component == i,]$ac_variable == "life_expectancy_scaled")]
d06 <- concentration[which(ac_variables[ac_variables$ac_component == i,]$ac_variable == "prop_labor_scaled")]
} else {
d04 <- 1
d05 <- 1
d06 <- 1
}
# learning
if (i == "learning") {
d07 <- concentration[which(ac_variables[ac_variables$ac_component == i,]$ac_variable == "hci_scaled")]
} else {
d07 <- 1
}
# social organization
if (i == "organization") {
d08 <- concentration[which(ac_variables[ac_variables$ac_component == i,]$ac_variable == "gov_effectiveness_scaled")]
d09 <- concentration[which(ac_variables[ac_variables$ac_component == i,]$ac_variable == "fsc_scaled")]
d10 <- concentration[which(ac_variables[ac_variables$ac_component == i,]$ac_variable == "rol_scaled")]
} else {
d08 <- 1
d09 <- 1
d10 <- 1
}
ac_simulated <- e_s_ac_long %>%
mutate(value_scaled = case_when(
# Assets
i == "assets" &
ac_variable == "gdp_scaled" ~ value * d01,
i != "assets" &
ac_variable == "gdp_scaled" ~ value * 1 / num_asse_vars,
i == "assets" &
ac_variable == "gdp_trade_scaled" ~ value * d02,
i != "assets" &
ac_variable == "gdp_trade_scaled" ~ value * 1 / num_asse_vars,
i == "assets" &
ac_variable == "sanitation_scaled" ~ value * d03,
i != "assets" &
ac_variable == "sanitation_scaled" ~ value * 1 / num_asse_vars,
# flexibility
i == "flexilbity" &
ac_variable == "supermarkets_scaled" ~ value * d04,
i != "flexilbity" &
ac_variable == "supermarkets_scaled" ~ value * 1 / num_flex_vars,
i == "flexilbity" &
ac_variable == "life_expectancy_scaled" ~ value * d05,
i != "flexilbity" &
ac_variable == "life_expectancy_scaled" ~ value * 1 / num_flex_vars,
i == "flexilbity" &
ac_variable == "prop_labor_scaled" ~ value * d06,
i != "flexilbity" &
ac_variable == "prop_labor_scaled" ~ value * 1 / num_flex_vars,
# learning
i == "learning" &
ac_variable == "hci_scaled" ~ value * d07,
i != "learning" &
ac_variable == "hci_scaled" ~ value * 1 / num_learn_vars,
# social organization
i == "organization" &
ac_variable == "gov_effectiveness_scaled" ~ value * d08,
i != "organization" &
ac_variable == "gov_effectiveness_scaled" ~ value * 1 / num_org_vars,
i == "organization" &
ac_variable == "fsc_scaled" ~ value * d09,
i != "organization" &
ac_variable == "fsc_scaled" ~ value * 1 / num_org_vars,
i == "organization" &
ac_variable == "rol_scaled" ~ value * d10,
i != "organization" &
ac_variable == "rol_scaled" ~ value * 1 / num_org_vars)
) %>%
filter(scenario == "2030ssp126") %>%
group_by(consumer_iso3c) %>%
summarize(adaptive_capacity = sum(value_scaled),
adaptive_capacity = adaptive_capacity * 0.5) %>%
filter(!is.na(adaptive_capacity))
# Calculate simulated vulnerability
risk_simulated <- left_join(e_s_ac_data, ac_simulated, by = "consumer_iso3c") %>%
filter(!is.na(adaptive_capacity),
scenario == "2030ssp126") %>%
mutate(across(c(change_in_stock, importance_on_protein_cons,
foreign_dependency), ~ ( . - min(., na.rm = TRUE) ) / ( max(., na.rm = TRUE) - min(., na.rm = TRUE) ), .names = "{.col}_scaled")) %>%
group_by(consumer_iso3c) %>%
mutate(importance_on_protein_cons_scaled = importance_on_protein_cons * 0.5,
foreign_dependency_scaled = foreign_dependency_scaled * 0.5,
sensitivity = sum(importance_on_protein_cons_scaled,foreign_dependency_scaled, na.rm = TRUE)) %>%
filter(!is.na(foreign_dependency_scaled),
!is.na(importance_on_protein_cons_scaled)) %>%
rename(exposure = "change_in_stock_scaled") %>%
select(consumer_iso3c, exposure, sensitivity, adaptive_capacity) %>%
mutate(adaptive_capacity = adaptive_capacity) %>%
mutate(vulnerability = (exposure + sensitivity) - adaptive_capacity)
t_test <- t.test(risk_simulated$vulnerability, risk$vulnerability)
pval <- t_test$p.value
# print(pval)
# print(concentration)
}
if (j == 1) {
cat(sprintf("*** AC VARIABLE COMPONENT: %s ***\n", str_to_upper(i)))
}
# Check for variables not sensitive
if ((modifier + 50) / ((modifier + 50) + (50 * (num_vars - 1))) > 0.90) {
cat(sprintf(
"##### Variable: %s is not sensitive #####\n",
ac_variables[ac_variables$ac_component == i, ]$ac_variable[j]
))
if (j == num_vars) {
cat("\n")
}
}
# For sensitive variables
if (pval < 0.05) {
cat("\n")
cat(
sprintf(
"Variable: %s was sensitive with the following parameters:\n",
ac_variables[ac_variables$ac_component == i, ]$ac_variable[j]
)
)
# Print Dirichlet concentration parameters
cat("Dirichlet concentration parameters:\n")
print(concentration)
# Print Dirichlet proportions
cat("Dirichlet proportions:\n")
print(round(dirichlet_probs, 2))
cat("\n")
if (!(which(i == ac_components$ac_component) == 4 & j == num_vars)) {
cat("---\n")
}
# Add spacing between components if it's the last variable
if (j == num_vars) {
cat("\n")
}
}
}
}
install.packages("bookdown")
knitr::opts_chunk$set(echo = TRUE)
library(LaplacesDemon) # For Dirichlet distribution
library(tidyverse)
library(arrow)
e_s_ac_data <- read_parquet("../output/e_s_ac_data.parquet")
# Calculate adaptive capacity across variables
adaptive_capacity_calcs <- e_s_ac_data %>%
mutate(across(c(gdp, gdp_trade, sanitation,
supermarkets, life_expectancy, prop_labor, hci, gov_effectiveness,
fsc, rol), ~ ( . - min(., na.rm = TRUE) ) / ( max(., na.rm = TRUE) - min(., na.rm = TRUE) ), .names = "{.col}_scaled")) %>% # scale variables using min/max scaling
mutate(gdp_scaled = gdp_scaled * (1/3), # Weight each AC variable within AC component (will add to 4 for country if each value is it's max value)
gdp_trade_scaled = gdp_trade_scaled * (1/3),
sanitation_scaled = sanitation_scaled * (1/3),
supermarkets_scaled = supermarkets_scaled * (1/3),
life_expectancy_scaled = life_expectancy_scaled * (1/3),
prop_labor_scaled = prop_labor_scaled * (1/3),
hci_scaled = hci * 1,
gov_effectiveness_scaled = gov_effectiveness_scaled * (1/3),
fsc_scaled = fsc_scaled * (1/3),
rol_scaled = rol_scaled * (1/3)) %>%
distinct(consumer_iso3c, .keep_all = TRUE) %>%
group_by(consumer_iso3c) %>%
mutate(adaptive_capacity = sum(gdp_scaled, gdp_trade_scaled,
sanitation_scaled, supermarkets_scaled,
life_expectancy_scaled,
prop_labor_scaled,
hci_scaled,
gov_effectiveness_scaled,
fsc_scaled,
rol_scaled)) %>%
mutate(adaptive_capacity = adaptive_capacity * 0.5) %>% # Divide by two, meaning max value would now = two, which would nullify max level expsorue + sensitivity
filter(!is.na(adaptive_capacity)) %>%
select(consumer_iso3c, adaptive_capacity)
# Calculate vulernability (Risk = Exposure + Risk - Adaptive Capacity)
risk <- left_join(e_s_ac_data, adaptive_capacity_calcs, by = "consumer_iso3c") %>%
filter(!is.na(adaptive_capacity),
scenario == "2030ssp126") %>%
mutate(across(c(change_in_stock, importance_on_protein_cons,
foreign_dependency), ~ ( . - min(., na.rm = TRUE) ) / ( max(., na.rm = TRUE) - min(., na.rm = TRUE) ), .names = "{.col}_scaled")) %>%
group_by(consumer_iso3c) %>%
mutate(importance_on_protein_cons_scaled = importance_on_protein_cons * 0.5,
foreign_dependency_scaled = foreign_dependency_scaled * 0.5,
sensitivity = sum(importance_on_protein_cons_scaled,foreign_dependency_scaled, na.rm = TRUE)) %>%
filter(!is.na(foreign_dependency_scaled),
!is.na(importance_on_protein_cons_scaled)) %>%
rename(exposure = "change_in_stock_scaled") %>%
select(consumer_iso3c, exposure, sensitivity, adaptive_capacity) %>%
mutate(adaptive_capacity = adaptive_capacity) %>%
mutate(vulnerability = (exposure + sensitivity) - adaptive_capacity)
# Map vulnerability
risk %>%
rename(eez_iso3c = "consumer_iso3c") %>%
create_map(fill = "vulnerability") +
labs(fill = "Vulnerability score")
knitr::opts_chunk$set(echo = TRUE)
# General data cleaning + visualization + analysis packages
library(tidyverse)
library(ggformula)
library(countrycode)
library(data.table)
library(arrow)
library(cowplot)
library(RColorBrewer)
# Map creation
library(rnaturalearth)
library(rnaturalearthdata)
library(sf)
# Load in packages needed for ARTIS
library(devtools)
library(tidytext)
# devtools::install_github("davidsjoberg/ggsankey")
library(ggsankey)
# devtools::install_github("Seafood-Globalization-Lab/exploreARTIS@v1.0.0", dependencies = TRUE)
library(exploreARTIS)
# For Packages for fishbase
# remotes::install_github("cboettig/duckdbfs", force = TRUE)
# remotes::install_github("ropensci/rfishbase")
library(duckdbfs)
library(rfishbase)
# Get list of files for future projections on EACH species
file_names <- list.files(path = "../data/species/", pattern = "\\.csv", full.names = TRUE)
# Obtain species lookup key (will be used in loop join)
species_names <- fread("../data/dbem_spp_list.csv")
# Read in eez lat long extents (use to identify eez territories by species distribution in fishbase)
eez_extents <- fread("../data/eez_lat_long_extents.csv")
# initiate empty df
df <- data.frame()
for (i in 1:length(file_names)) {
# Read in i'th species file in loop
df_i <- fread(file_names[i])
# rename species variable to taxon_key to a join can work
df_i <- df_i %>%
rename(taxon_key = "species")
df_i <- left_join(df_i, species_names, by = "taxon_key")
# Select only certain variables for certain years and pivot wider
df_i <- df_i %>%
filter(year %in% c(2030, 2050, 2100)) %>%
mutate(year_ssp = paste0(year, ssp), sciname = str_to_lower(paste(genus, species))) %>%
select(sciname, eez_name, year_ssp, mean_per_change) %>%
pivot_wider(names_from = year_ssp, values_from = mean_per_change)
# Combine all species observations into one file
df <- df %>%
bind_rows(df_i)
}
# Obtain previous state on dataframe (to get unmatched)
df_preprocessed <- df
# Convert eez variable countries to iso3c codes
for (i in 1:length(df$eez_name)) {
df$eez_name[i] <- countrycode(df$eez_name[i], origin = 'country.name', destination = 'iso3c')
}
# Rename country variable so it can be joined
df <- df %>%
rename(eez_iso3c = "eez_name")
# # See how many countries were not matches
# sum(is.na(df$eez_iso3c))
#
# # Number of countries that properly got matched
# length(unique(df$eez_iso3c))
#
# # Unmatched countries via countrycode
unmatched_countries <- unique(df_preprocessed$eez_name[c(which(is.na(df$eez_iso3c)))])
#
# # Only keep rows in preprocessed dataset that are unmatched countries
# df_preprocessed <- df_preprocessed %>%
#   filter(eez_name %in% unmatched_countries)
#
# df_postprocessed <- df_preprocessed
#
# # Get country names out of parenthesis with string detects
# for (i in seq_along(df_preprocessed$eez_name)) {
#
#   if (xor(str_detect(df_preprocessed$eez_name[i], "\\("), str_detect(df_preprocessed$eez_name[i], "Yemen"))) {
#     split_string <- str_split(df_preprocessed$eez_name[i], "\\(")
#     split_string <- str_split(split_string[[1]][2], "\\)")
#     df_postprocessed$eez_name[i] <- split_string[[1]][1]
#   }
#   else if (str_detect(df_preprocessed$eez_name[i], "Yemen")) {
#     df_postprocessed$eez_name[i] <- str_split(df_preprocessed$eez_name[i], "\\ \\(")[[1]][1]
#   }
#
# }
#
# # Convert previously unmatched countries to iso3c
# for (i in 1:length(df$eez_name)) {
#   df_postprocessed$eez_name[i] <- countrycode(df_postprocessed$eez_name[i], origin = 'country.name', destination = 'iso3c')
# }
#
# # Rename country variable so it can be joined
# df_postprocessed <- df_postprocessed %>%
#   rename(eez_iso3c = "eez_name")
#
# # Add in unmatched countries (now all countries are matched)
# df <- bind_rows(df, df_postprocessed)
unmatched_countries
knitr::opts_chunk$set(echo = TRUE)
# Read in fishbase/sealifebase data
fb_presences_country <- read_parquet("../data/fb_slb_data/fb_presences_country.parquet")
fb_presences_country
fb_presences_country
ecosystem()
ecosystem("Alopias pelagicus")
ecosystem("Alopias pelagicus") %>%
str_dtect("pelagic")
ecosystem("Alopias pelagicus") %>%
str_detect("pelagic")
ecosystem("Alopias pelagicus")
ecosystem("Alopias pelagicus") %>%
which(str_detect("pelagic"))
ecosystem("Alopias pelagicus") %>%
str_detect("pelagic")
ecology("Alopias pelagicus")
ecology(species_list = c("Alopias pelagicus", "Ageneiosus ucayalensis"))
species_list <- load_taxa() %>%
pull(Species) %>%
ecology(species_list = c("Alopias pelagicus", "Ageneiosus ucayalensis"))
species_list <- load_taxa() %>%
pull(Species)
ecology(species_list = species_list) %>%
distinct(pelagic)
ecology(species_list = species_list)
ecology(species_list = species_list) %>%
distinct(Pelagic)
ecology(species_list = species_list) %>%
filter(Species == "Abramites eques") %>%
distinct(Pelagic)
species_list
ecology(species_list = species_list)
ecology(species_list = species_list) %>%
filter(Species == "Abramites eques") %>%
distinct(Mesopelagic)
x <- ecology(species_list = species_list) %>%
filter(Species == "Abramites eques")
colnames(x)
str_detect(colnames(x), "meso")
x
x
x <- ecology(species_list = species_list)
names(x)
x %>%
distinct(Mesopelagic)
x %>%
distinct(Epipelagic)
x %>%
count(Pelagic)
x %>%
count(Mesopelagic)
x %>%
count(Pelagic)
x %>%
count(Mesopelagic)
y <- species_by_ecosystem(species_list)
y <- species_by_ecosystem()
y <- species_by_ecosystem(ecosystem = "Pelagic")
y <- species_by_ecosystem(ecosystem = "Arctic")
y
y <- country(species_list)
colnames(y)
fisbase()
fishbase()
fishbase()
y
colnames(y)
y %>% distinct(Saltwater)
y
y %>% distinct(Saltwater)
x %>%
count(Pelagic)
x %>%
count(Mesopelagic)
# General data cleaning + visualization + analysis packages
library(tidyverse)
library(ggformula)
library(countrycode)
library(data.table)
library(arrow)
library(cowplot)
library(RColorBrewer)
# Map creation
library(rnaturalearth)
library(rnaturalearthdata)
library(sf)
# Load in packages needed for ARTIS
library(devtools)
library(tidytext)
# devtools::install_github("davidsjoberg/ggsankey")
library(ggsankey)
# devtools::install_github("Seafood-Globalization-Lab/exploreARTIS@v1.0.0", dependencies = TRUE)
library(exploreARTIS)
# Read in cosnumption data
consumption <- read_parquet("../data/example_consumption_eez_2024_12_06.parquet")
# Calcualte total foreign weight consumption
domestic_consumption_weights <- consumption %>%
group_by(consumer_iso3c, year) %>%
filter(consumer_iso3c == producer_iso3c) %>%
summarize(domestic_weight = sum(live_weight_t)) %>%
arrange(consumer_iso3c)
# Calculate total foreign weight consumption
foreign_consumption_weights <- consumption %>%
group_by(consumer_iso3c, year) %>%
filter(consumer_iso3c != producer_iso3c) %>%
summarize(foreign_weight = sum(live_weight_t)) %>%
arrange(consumer_iso3c)
# Calculate trade dependency
consumer_foreign_dependencies <- foreign_consumption_weights %>%
filter(consumer_iso3c %in% domestic_consumption_weights$consumer_iso3c) %>%
left_join(domestic_consumption_weights) %>%
group_by(consumer_iso3c, year) %>%
summarize(foreign_dependency = round(foreign_weight /
(foreign_weight +
domestic_weight), 3)) %>%
exploreARTIS::add_region("consumer_iso3c", region.col.name = "region")
# Find greatest / least changes in foreign dependency
# Largest increase = ISL; largest decrease = NZL
consumer_foreign_dependencies %>%
# Filter to get only rows for 1996 and 2019
filter(year %in% c(1996, 2019)) %>%
# Spread data so that each year gets its own column
pivot_wider(names_from = year, values_from = foreign_dependency) %>%
# Calculate the difference between 1996 and 2019
mutate(diff_2019_1996 = `2019` - `1996`) %>%
# Select columns as needed
select(consumer_iso3c, diff_2019_1996) %>%
arrange(-diff_2019_1996)
# Look at average change in foreign dependency from 2015-2019
foreign_dependency_slopes <- consumer_foreign_dependencies %>%
split(.$consumer_iso3c) %>%
map(~lm(foreign_dependency~year, data = .x)) %>%
map_df(broom::tidy, .id = 'consumer_iso3c') %>%
filter(term == 'year') %>%
select(consumer_iso3c, "slope" = "estimate") %>%
mutate(fill_color = case_when(
slope > 0 ~ "1",
slope < 0 ~ "0",
TRUE ~ NA
)) %>%
add_region(col = "consumer_iso3c", region.col.name = "region")
foreign_dependency_slopes %>%
filter(!is.na(region)) %>%
ggplot(aes(x = reorder(consumer_iso3c, -slope),
y = slope, fill = region)) +
geom_col() +
theme_minimal_hgrid() +
theme(axis.text.x = element_blank()) +
labs(x = "Consuming country", y = "Slope between foreign\ndependency and time", fill = "Region") +
scale_fill_manual(values = artis_palette(7)) +
ylim
foreign_dependency_slopes %>%
filter(!is.na(region)) %>%
ggplot(aes(x = reorder(consumer_iso3c, -slope),
y = slope, fill = region)) +
geom_col() +
theme_minimal_hgrid() +
theme(axis.text.x = element_blank()) +
labs(x = "Consuming country", y = "Slope between foreign\ndependency and time", fill = "Region") +
scale_fill_manual(values = artis_palette(7))
foreign_dependency_slopes %>%
filter(!is.na(region)) %>%
ggplot(aes(x = reorder(consumer_iso3c, -slope),
y = slope, fill = region)) +
geom_col() +
theme_minimal_hgrid() +
theme(axis.text.x = element_blank()) +
labs(x = "Consuming country", y = "Slope between foreign\ndependency and time", fill = "Region") +
scale_fill_manual(values = artis_palette(7)) +
ylim(-0.5, 0.5)
foreign_dependency_slopes %>%
filter(!is.na(region)) %>%
ggplot(aes(x = reorder(consumer_iso3c, -slope),
y = slope, fill = region)) +
geom_col() +
theme_minimal_hgrid() +
theme(axis.text.x = element_blank()) +
labs(x = "Consuming country", y = "Slope between foreign\ndependency and time", fill = "Region") +
scale_fill_manual(values = artis_palette(7)) +
ylim(-0.05, 0.05)
knitr::opts_chunk$set(echo = TRUE)
library(LaplacesDemon) # For Dirichlet distribution
library(tidyverse)
library(arrow)
e_s_ac_data
e_s_ac_data <- read_parquet("../output/e_s_ac_data.parquet")
e_s_ac_data
gc()
readRDS("../x.rds")
readRDS("x.rds")
